{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets,transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainSet = datasets.MNIST('MNIST_data/', download = True, train = True, transform = transform)\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(trainSet, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images torch.Size([64, 1, 28, 28])\n",
      "Labels torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "#trainLoader batch_size is 64 which means we get 64 images in one iteration.\n",
    "# Images tensor of size (64, 1, 28, 28)\n",
    "dataIter = iter(trainLoader)\n",
    "images, labels = dataIter.next()\n",
    "\n",
    "print('Images',images.shape)\n",
    "print('Labels', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff13650d9e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at one of the images\n",
    "plt.imshow(images[1].numpy().squeeze(), cmap = 'Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to convert each batch of images of size 64*28*28 into 64*784 tensor\n",
    "# This is called flattening of 2D images into 1D vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the architecture of our neural network will be 784 inputs\n",
    "#and say around 256 hidden units and there are 10 output units\n",
    "#as possible outcomes are from  0 to 9 ''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 28*28\n",
    "n_hidden = 256\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "# initialize weights\n",
    "w1 = torch.randn(n_input, n_hidden)\n",
    "b1 = torch.randn(n_hidden)\n",
    "\n",
    "w2 = torch.randn(n_hidden, n_output)\n",
    "b2 = torch.randn(n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = images.view(images.shape[0], -1) # can also use images.view(images.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = activation(torch.mm(inputs, w1) + b1)\n",
    "out = torch.mm(h, w2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1.2179,  -6.5837,  -2.4413,  -0.5424,  17.6615, -27.8658,  13.4763,\n",
      "          -8.7140,  -3.9037,   8.7783],\n",
      "        [ -1.1662, -12.0211,  -5.7173,  11.0091,   9.4962, -32.3793,  20.0943,\n",
      "          -9.5718, -10.8663,  11.3659],\n",
      "        [  4.4553, -19.1379,  -5.5847,  11.2755,  11.9706, -35.9600,  10.9073,\n",
      "         -14.7027,   2.5096,  11.5156],\n",
      "        [ -8.6538, -12.9074,  12.7575,  17.8917,   6.7756, -29.6555,  19.9782,\n",
      "          -6.4583,  -5.7498,   6.8336],\n",
      "        [ -8.8747,  -7.0710, -11.0779,  14.2408,   6.2115, -21.2958,  12.1991,\n",
      "         -16.9687,  -4.0054,   4.7940],\n",
      "        [ -9.0128, -16.2877,  -8.7013,   5.1623,   6.2635, -21.6180,  15.5407,\n",
      "          -8.3856,   3.0763,  17.3581],\n",
      "        [  3.5917, -10.6874,  -3.3736,   9.1009,  12.5147, -20.9266,  16.2009,\n",
      "          -7.8655,  -6.0780,  10.0218],\n",
      "        [  3.3714, -10.6569,   0.1040,   2.8975,  11.8763, -29.8030,  15.8402,\n",
      "          -7.3140,   6.2112,  11.3027],\n",
      "        [ -3.3062, -17.3594,   1.2037,   5.3320,  13.8940, -27.6208,   8.9773,\n",
      "          -7.2040,   3.2140,   7.7191],\n",
      "        [ -1.2260, -11.6783,  -7.4495,   4.3485,   4.7401, -14.1222,  22.6591,\n",
      "          -5.8776,  -7.8243,  12.8308],\n",
      "        [ -3.3667,  -4.9796,   2.3798,  -2.2910,  14.3384, -29.4570,  16.7040,\n",
      "          -5.6963,  -6.9815,   7.0178],\n",
      "        [  3.6246,  -8.5287,   4.1485,   1.7299,   9.7038, -26.7233,  14.2257,\n",
      "         -11.3243,   4.7981,  11.8480],\n",
      "        [  6.8310, -15.3236,  -0.5002,   0.2053,   9.5697, -26.7437,  12.3055,\n",
      "          -4.6940,  -9.8893,  11.5965],\n",
      "        [  4.9017, -14.9014,  -6.7096,   8.6902,  13.5773, -25.9773,   3.1366,\n",
      "          -2.0498,  -3.2378,  22.5718],\n",
      "        [ -2.0143,  -4.7548,   7.9747,   2.5339,  13.9966, -24.7046,  17.6730,\n",
      "          -6.1570,   3.0687,  19.3139],\n",
      "        [ -7.2812,  -9.9809,   0.8111,  -2.7912,  17.2732, -23.4283,   9.1221,\n",
      "         -10.4302,   4.9220,   9.4005],\n",
      "        [ -2.0392, -16.3024,   1.1997,  11.7580,   9.0994, -25.9522,   7.9967,\n",
      "          -8.8313,  -8.8610,  -0.1046],\n",
      "        [ -1.4290, -23.8867,  -9.2945,   7.8091,   0.6627, -16.8763,   5.9803,\n",
      "         -12.4233,   0.6244,  12.6330],\n",
      "        [-14.3262,  -6.0819,  12.2447,  -2.2731,  14.2590, -32.5554,   7.0628,\n",
      "          -3.7902,  10.0265,   2.4531],\n",
      "        [  1.5652, -13.8932,  -7.8083,   0.7454,  13.0667, -22.1700,   4.8892,\n",
      "         -14.1834, -10.5555,  25.5566],\n",
      "        [ -3.8251, -14.4937,   1.1855,  18.8464,  10.7696, -27.4587,  12.8645,\n",
      "         -15.0415,  -2.9787,   7.0944],\n",
      "        [  3.9897, -10.7948,   6.3573,   7.4240,  17.1939, -33.7878,  11.1177,\n",
      "         -10.5167,   3.1884,  11.5194],\n",
      "        [  4.5371,  -9.4311,  -0.9016,  10.8718,  15.7919, -27.0722,   9.0352,\n",
      "         -12.5036,   0.3504,  11.1850],\n",
      "        [ -1.7983, -16.7962,   2.5805,   2.0214,  23.8202, -24.9460,  14.9384,\n",
      "           5.0073,  -4.7365,  13.0328],\n",
      "        [  0.3029, -13.6597,  -1.6266,  13.7607,   2.7443, -16.4460,  11.7752,\n",
      "          -9.9117,  -0.4135,   1.5407],\n",
      "        [ 10.1917, -20.1549,   0.0123,  -1.4596,  11.5786, -32.3898,   8.2657,\n",
      "         -13.0033,   4.0524,   8.3547],\n",
      "        [ -5.0979, -12.3593,   2.6650,   2.0033,   8.6703, -22.9487,  11.6542,\n",
      "          -7.9494,  -6.9588,   8.8374],\n",
      "        [ -5.6776, -10.5555,  -2.4266,  -7.9819,  13.2576, -24.5170,   5.2661,\n",
      "         -11.0847,  -6.2238,   6.1550],\n",
      "        [  5.5974, -12.9046,  -2.0834,   2.5926,  17.5407, -25.7957,   9.6811,\n",
      "          -5.2536,   0.1102,  12.4929],\n",
      "        [  0.9565, -13.2378,   2.9499,   7.0443,  13.3238, -34.7706,   8.6487,\n",
      "         -11.7435,  -2.4646,   2.7011],\n",
      "        [ -4.6823, -13.9154,   7.0283,  11.8484,   9.6889, -33.3377,   9.0210,\n",
      "         -11.4129,  -6.7294,   6.0603],\n",
      "        [ -3.0430,  -1.9772,   3.3979,   4.4925,   5.9910, -16.1674,  21.0764,\n",
      "          -7.9280,  -0.1945,  10.1706],\n",
      "        [ -4.8205, -15.6790,  -0.9551,  13.7337,  12.2388, -19.6632,  14.2084,\n",
      "         -12.1598, -13.2157,   0.6265],\n",
      "        [  7.0415, -20.6303,  -8.9927,   2.7884,   8.3724, -21.3539,   8.9301,\n",
      "          -8.9245,   4.5089,  17.8127],\n",
      "        [ -8.1021,  -0.6749,   4.3096,   8.7730,   2.2751, -22.6622,  14.0445,\n",
      "         -20.3894,  -1.1016,   7.0137],\n",
      "        [ -3.2122, -11.9932,  -1.1558,  -3.0023,  11.0000, -38.0762,  18.5906,\n",
      "          -5.2832,   7.5090,  17.3401],\n",
      "        [ -7.7989,  -7.8540,   4.1394,   8.5950,  12.1786, -29.2622,   7.2671,\n",
      "         -11.1367,  -4.5470,  17.6648],\n",
      "        [  2.1323, -12.6168,   2.5260,   2.0870,  18.0665, -36.9881,  16.8247,\n",
      "          -9.3085,  -2.4629,  10.4080],\n",
      "        [  1.0372, -11.2525,  -2.0415,   6.6675,   7.5358, -20.5105,   2.1450,\n",
      "          -8.2752,  -0.6057,   8.8028],\n",
      "        [ -4.2642, -10.1496,  -5.8887,  -6.9940,   7.8627, -32.1865,  13.5545,\n",
      "         -10.0340,  -6.4650,  19.3688],\n",
      "        [ -3.1152,  -9.4836,  -2.8059,   6.6485,   6.1982, -28.0310,   8.4258,\n",
      "          -2.5364,  -3.5161,  12.0694],\n",
      "        [ -1.1813,   2.3327,   1.2353,  -0.6222,   7.3092, -17.9285,  12.3121,\n",
      "         -10.1931,   2.7360,   5.4048],\n",
      "        [ -3.5836, -13.9875,  10.7650,  12.7905,   7.1232, -28.6587,  18.9145,\n",
      "          -7.2144,  -6.4577,   6.1550],\n",
      "        [  4.2196,  -6.2812,   0.5330,  -0.9671,  12.7068, -26.2658,  15.2258,\n",
      "          -5.9855,  -2.8764,  15.1123],\n",
      "        [ -8.5462,  -6.3765,  10.6099,  -6.6294,  15.8197, -33.1683,  12.1239,\n",
      "         -13.4436,   1.4948,  13.1090],\n",
      "        [ -4.6936,  -5.9047,   3.4066,   2.5820,  13.3287, -33.4773,  11.7796,\n",
      "          -2.7281,  -2.3477,  16.7786],\n",
      "        [  5.1495,  -7.4624,   3.9775,   4.3612,   4.3873, -32.3821,   8.0202,\n",
      "          -6.4816,  -1.2935,  14.8611],\n",
      "        [ -5.3614, -12.8607,   0.9145,   5.8011,   8.5853, -28.1018,   6.7357,\n",
      "          -9.8543,  -4.8078,   5.2864],\n",
      "        [  2.6209, -13.6137,   5.4163,   6.5812,  14.3262, -35.6864,  10.0193,\n",
      "         -10.0165,  -1.0394,  10.4688],\n",
      "        [  3.7798, -14.1132,   4.9090,   7.0948,  10.5594, -26.2441,  19.5776,\n",
      "          -5.9431,  10.3371,  12.7482],\n",
      "        [-14.4601, -22.8793,  -6.1202,   7.7263,  10.0893, -14.5947,   5.6453,\n",
      "          -9.9725,  -5.3422,  12.6871],\n",
      "        [  3.6824,  -7.3666,  -4.2940,  14.4727,  11.6179, -28.6415,  21.8120,\n",
      "          -8.7431,  -1.5151,   7.1947],\n",
      "        [-12.1817,  -5.1896,   1.2806,  -3.2369,  15.0690, -30.7062,  10.8255,\n",
      "          -9.7995,   1.4768,  10.2914],\n",
      "        [  6.7208, -14.6381,  -8.3452,   1.6135,   3.3824, -22.8568,   9.5495,\n",
      "         -15.2747,   1.6787,  15.0365],\n",
      "        [ -2.2433, -10.5402,   2.7126,  13.6091,  15.0292, -35.6305,  16.9454,\n",
      "          -6.8314,  -0.8782,   8.2968],\n",
      "        [ -7.8245, -10.0516,   5.7349,   7.2152,   1.1187, -35.5756,  14.5573,\n",
      "          -4.3613,   3.9091,  20.6364],\n",
      "        [  4.0295, -12.1067,  -1.3587,   1.5891,   9.7546, -27.0173,  10.3648,\n",
      "         -13.9104,  -4.2769,   8.3462],\n",
      "        [ -6.3912, -10.1369,  -9.4277,   4.3155,   7.8329, -17.2952,   9.8992,\n",
      "          -5.9924,  -3.0428,  14.5682],\n",
      "        [ -2.4592, -13.5395,   9.4417,   6.1695,  13.0595, -28.5251,  12.9780,\n",
      "          -5.6520,  12.3795,   6.8969],\n",
      "        [  7.1636,  -8.5806,  -8.4881,  -2.2794,  15.5881, -22.8308,  12.4448,\n",
      "          -4.2349,   0.3505,   9.1921],\n",
      "        [ -7.1472, -17.9793,   6.2071,   6.9671,   2.3700, -27.0404,  11.3488,\n",
      "         -12.1704,  -1.1810,  20.9964],\n",
      "        [ -9.5036, -16.6902, -10.7719,   1.7013,   7.2446, -18.4824,  10.8689,\n",
      "          -7.4658,  -2.6618,  17.5603],\n",
      "        [ -1.0696,  -9.3849,  -2.2083,   6.9375,   5.2674, -23.8216,   8.8947,\n",
      "          -4.4492,  -4.0107,  16.6101],\n",
      "        [ -3.3487,  -9.6852,   6.2679,   4.3370,  10.6557, -33.4348,   4.4717,\n",
      "         -13.3852,  -1.4336,  16.2074]])\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get the probability of the image belonging to a particular class, we often use softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim = 1).view(-1, 1)\n",
    "\n",
    "probabilities = softmax(out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "print(probabilities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "print(probabilities.sum(dim = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building neural networks using PyTorch's 'nn' module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the above network using nn module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input to hidden layer transformation\n",
    "        self.hidden = nn.Linear(28*28, 256) # this does the linear transformation (Wx+b)\n",
    "        \n",
    "        #hidden layer to out layer transformation\n",
    "        self.out = nn.Linear(256, 10)\n",
    "        \n",
    "        # define activation functions\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.hidden(x)\n",
    "            x = self.sigmoid(x)\n",
    "            x = self.output(x)\n",
    "            x = self.softmax(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network()\n",
    "model # See model representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can define the network more concisely and clearly using 'torch.nn.functional' module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input to hidden layer transformation\n",
    "        self.hidden = nn.Linear(28*28, 256) # this does the linear transformation (Wx+b)\n",
    "        \n",
    "        #hidden layer to out layer transformation\n",
    "        self.out = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        x = F.softmax(self.out(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surya/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "prob = model.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a larger network with 'ReLU' activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_1 = nn.Linear(28*28, 128)\n",
    "        self.hidden_2 = nn.Linear(128, 64)\n",
    "        self.out = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden_1(x))\n",
    "        x = F.relu(self.hidden_2(x))\n",
    "        x = F.softmax(self.out(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LargeNetwork(\n",
       "  (hidden_1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (hidden_2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (out): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = LargeNetwork()\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_1 = model_1.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1024, 0.0938, 0.0887, 0.1260, 0.0863, 0.1006, 0.1113, 0.1093, 0.0888,\n",
       "         0.0925],\n",
       "        [0.1023, 0.0946, 0.0868, 0.1115, 0.0892, 0.1074, 0.1161, 0.1088, 0.0835,\n",
       "         0.0997],\n",
       "        [0.0940, 0.0891, 0.0794, 0.1203, 0.0924, 0.1023, 0.1133, 0.1167, 0.0928,\n",
       "         0.0997],\n",
       "        [0.1003, 0.0949, 0.0843, 0.1238, 0.0949, 0.1041, 0.1112, 0.1048, 0.0831,\n",
       "         0.0985],\n",
       "        [0.0967, 0.0921, 0.0882, 0.1171, 0.0869, 0.1024, 0.1120, 0.1110, 0.0903,\n",
       "         0.1034],\n",
       "        [0.1019, 0.1015, 0.0812, 0.1173, 0.0988, 0.0961, 0.1200, 0.1040, 0.0866,\n",
       "         0.0926],\n",
       "        [0.0958, 0.0979, 0.0852, 0.1259, 0.0878, 0.0960, 0.1121, 0.1115, 0.0929,\n",
       "         0.0950],\n",
       "        [0.1056, 0.0935, 0.0920, 0.1171, 0.0862, 0.1062, 0.1113, 0.1051, 0.0932,\n",
       "         0.0898],\n",
       "        [0.0981, 0.0946, 0.0874, 0.1244, 0.0883, 0.1066, 0.1141, 0.1074, 0.0886,\n",
       "         0.0905],\n",
       "        [0.0967, 0.0945, 0.0785, 0.1211, 0.0997, 0.1049, 0.1140, 0.1050, 0.0890,\n",
       "         0.0967],\n",
       "        [0.0925, 0.0958, 0.0918, 0.1273, 0.0847, 0.1056, 0.1056, 0.1115, 0.0869,\n",
       "         0.0983],\n",
       "        [0.0922, 0.0969, 0.0828, 0.1328, 0.0856, 0.0958, 0.1099, 0.1100, 0.0919,\n",
       "         0.1021],\n",
       "        [0.1018, 0.1012, 0.0930, 0.1218, 0.0898, 0.1052, 0.1118, 0.1065, 0.0849,\n",
       "         0.0839],\n",
       "        [0.0898, 0.0978, 0.0856, 0.1311, 0.0876, 0.1063, 0.1117, 0.1077, 0.0890,\n",
       "         0.0935],\n",
       "        [0.0967, 0.0970, 0.0880, 0.1215, 0.0873, 0.1009, 0.1116, 0.1091, 0.0918,\n",
       "         0.0960],\n",
       "        [0.0977, 0.0957, 0.0788, 0.1285, 0.0907, 0.1024, 0.1148, 0.1129, 0.0891,\n",
       "         0.0894],\n",
       "        [0.0984, 0.0931, 0.0871, 0.1241, 0.0944, 0.1089, 0.1094, 0.1012, 0.0850,\n",
       "         0.0985],\n",
       "        [0.0990, 0.0948, 0.0818, 0.1215, 0.0941, 0.0975, 0.1170, 0.1095, 0.0864,\n",
       "         0.0985],\n",
       "        [0.0977, 0.0990, 0.0891, 0.1240, 0.0935, 0.1064, 0.1059, 0.1052, 0.0855,\n",
       "         0.0936],\n",
       "        [0.1005, 0.0981, 0.0863, 0.1140, 0.0837, 0.1025, 0.1210, 0.1126, 0.0855,\n",
       "         0.0959],\n",
       "        [0.0971, 0.0977, 0.0890, 0.1153, 0.0968, 0.1039, 0.1109, 0.1044, 0.0848,\n",
       "         0.1002],\n",
       "        [0.1016, 0.0950, 0.0895, 0.1247, 0.0869, 0.1068, 0.1129, 0.1059, 0.0880,\n",
       "         0.0887],\n",
       "        [0.1004, 0.0961, 0.0864, 0.1282, 0.0942, 0.1090, 0.1149, 0.1032, 0.0831,\n",
       "         0.0846],\n",
       "        [0.0987, 0.0947, 0.0897, 0.1175, 0.0813, 0.1006, 0.1139, 0.1126, 0.0914,\n",
       "         0.0996],\n",
       "        [0.0977, 0.0921, 0.0872, 0.1187, 0.0944, 0.1050, 0.1082, 0.1020, 0.0832,\n",
       "         0.1115],\n",
       "        [0.0984, 0.0972, 0.0809, 0.1188, 0.0880, 0.0996, 0.1168, 0.1102, 0.0963,\n",
       "         0.0937],\n",
       "        [0.0984, 0.0921, 0.0851, 0.1284, 0.0941, 0.1067, 0.1121, 0.1072, 0.0840,\n",
       "         0.0920],\n",
       "        [0.0978, 0.0973, 0.0814, 0.1332, 0.0865, 0.1044, 0.1183, 0.1054, 0.0851,\n",
       "         0.0907],\n",
       "        [0.1029, 0.0973, 0.0868, 0.1191, 0.0916, 0.0998, 0.1148, 0.1072, 0.0893,\n",
       "         0.0911],\n",
       "        [0.0987, 0.0942, 0.0874, 0.1248, 0.0904, 0.1081, 0.1105, 0.1035, 0.0895,\n",
       "         0.0929],\n",
       "        [0.0987, 0.0930, 0.0862, 0.1206, 0.0862, 0.1034, 0.1133, 0.1074, 0.0899,\n",
       "         0.1013],\n",
       "        [0.0982, 0.1026, 0.0873, 0.1164, 0.0867, 0.1067, 0.1107, 0.1112, 0.0894,\n",
       "         0.0908],\n",
       "        [0.0969, 0.0934, 0.0900, 0.1182, 0.0911, 0.1066, 0.1094, 0.1062, 0.0857,\n",
       "         0.1025],\n",
       "        [0.0924, 0.0929, 0.0734, 0.1244, 0.0918, 0.0992, 0.1194, 0.1093, 0.0974,\n",
       "         0.0998],\n",
       "        [0.0927, 0.0927, 0.0874, 0.1179, 0.0834, 0.1054, 0.1082, 0.1150, 0.0931,\n",
       "         0.1042],\n",
       "        [0.1012, 0.1009, 0.0794, 0.1363, 0.0910, 0.1030, 0.1169, 0.1016, 0.0874,\n",
       "         0.0823],\n",
       "        [0.0997, 0.0909, 0.0900, 0.1244, 0.0865, 0.1154, 0.1081, 0.1029, 0.0874,\n",
       "         0.0945],\n",
       "        [0.1087, 0.0970, 0.0923, 0.1275, 0.0931, 0.1018, 0.1102, 0.1015, 0.0827,\n",
       "         0.0853],\n",
       "        [0.1051, 0.0920, 0.0889, 0.1121, 0.0879, 0.1024, 0.1152, 0.1123, 0.0850,\n",
       "         0.0989],\n",
       "        [0.1004, 0.0974, 0.0860, 0.1214, 0.0907, 0.1006, 0.1113, 0.1162, 0.0895,\n",
       "         0.0864],\n",
       "        [0.0969, 0.0915, 0.0787, 0.1211, 0.0937, 0.1013, 0.1152, 0.1159, 0.0928,\n",
       "         0.0929],\n",
       "        [0.0940, 0.0929, 0.0792, 0.1245, 0.0845, 0.1029, 0.1060, 0.1145, 0.0991,\n",
       "         0.1024],\n",
       "        [0.1019, 0.0924, 0.0839, 0.1229, 0.0947, 0.0988, 0.1134, 0.0992, 0.0865,\n",
       "         0.1065],\n",
       "        [0.0958, 0.0936, 0.0755, 0.1281, 0.0939, 0.0992, 0.1217, 0.1120, 0.0890,\n",
       "         0.0912],\n",
       "        [0.1018, 0.0908, 0.0856, 0.1246, 0.0912, 0.1059, 0.1113, 0.1080, 0.0931,\n",
       "         0.0876],\n",
       "        [0.0973, 0.0988, 0.0867, 0.1169, 0.0880, 0.1037, 0.1191, 0.1073, 0.0896,\n",
       "         0.0926],\n",
       "        [0.0954, 0.0956, 0.0888, 0.1327, 0.0881, 0.1086, 0.1057, 0.1115, 0.0896,\n",
       "         0.0840],\n",
       "        [0.1001, 0.0940, 0.0898, 0.1234, 0.0893, 0.1086, 0.1109, 0.1051, 0.0874,\n",
       "         0.0915],\n",
       "        [0.1008, 0.0933, 0.0883, 0.1216, 0.0880, 0.1079, 0.1131, 0.1051, 0.0897,\n",
       "         0.0921],\n",
       "        [0.0992, 0.0933, 0.0898, 0.1201, 0.0902, 0.1068, 0.1073, 0.1065, 0.0832,\n",
       "         0.1035],\n",
       "        [0.0958, 0.0907, 0.0812, 0.1204, 0.0939, 0.0968, 0.1204, 0.1169, 0.0899,\n",
       "         0.0941],\n",
       "        [0.1011, 0.0989, 0.0861, 0.1132, 0.0822, 0.0985, 0.1217, 0.1109, 0.0918,\n",
       "         0.0956],\n",
       "        [0.0956, 0.1010, 0.0855, 0.1313, 0.0845, 0.1055, 0.1138, 0.1044, 0.0848,\n",
       "         0.0936],\n",
       "        [0.0901, 0.0937, 0.0773, 0.1263, 0.0900, 0.1006, 0.1119, 0.1172, 0.0910,\n",
       "         0.1019],\n",
       "        [0.0984, 0.0955, 0.0895, 0.1301, 0.0873, 0.1068, 0.1106, 0.1045, 0.0848,\n",
       "         0.0924],\n",
       "        [0.0987, 0.0971, 0.0893, 0.1274, 0.0889, 0.1044, 0.1071, 0.1075, 0.0817,\n",
       "         0.0978],\n",
       "        [0.0940, 0.0934, 0.0852, 0.1221, 0.0860, 0.0991, 0.1094, 0.1181, 0.0999,\n",
       "         0.0929],\n",
       "        [0.0980, 0.0890, 0.0751, 0.1248, 0.0886, 0.1001, 0.1161, 0.1193, 0.0938,\n",
       "         0.0952],\n",
       "        [0.0990, 0.0954, 0.0818, 0.1220, 0.0943, 0.1007, 0.1123, 0.1062, 0.0926,\n",
       "         0.0958],\n",
       "        [0.0952, 0.0955, 0.0808, 0.1241, 0.0893, 0.0977, 0.1141, 0.1164, 0.0931,\n",
       "         0.0937],\n",
       "        [0.1033, 0.1026, 0.0903, 0.1216, 0.0820, 0.1059, 0.1129, 0.1116, 0.0810,\n",
       "         0.0889],\n",
       "        [0.0978, 0.0906, 0.0784, 0.1220, 0.0910, 0.0969, 0.1145, 0.1187, 0.0950,\n",
       "         0.0950],\n",
       "        [0.0949, 0.0969, 0.0870, 0.1250, 0.0843, 0.1026, 0.1164, 0.1118, 0.0864,\n",
       "         0.0946],\n",
       "        [0.0998, 0.0919, 0.0880, 0.1310, 0.0901, 0.1065, 0.1062, 0.1112, 0.0874,\n",
       "         0.0878]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model for each data iteration gives the predictions for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
